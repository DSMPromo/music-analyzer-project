import React, { useState, useRef, useEffect, useCallback } from 'react';
import { Scale, Chord, Note } from 'tonal';

// ==================== TYPES ====================
interface ChordInfo {
  symbol: string;
  root: string;
  quality: string;
  confidence: number;
  notes: string[];
}

interface AnalysisResult {
  key: string;
  tempo: number | null;
  chords: ChordInfo[];
  structure: { name: string; startTime: number; endTime: number }[];
}

// ==================== MAIN APP ====================
export default function MusicAnalyzerApp() {
  // State
  const [audioContext, setAudioContext] = useState<AudioContext | null>(null);
  const [analyser, setAnalyser] = useState<AnalyserNode | null>(null);
  const [sourceNode, setSourceNode] = useState<AudioNode | null>(null);
  const [isPlaying, setIsPlaying] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [currentChord, setCurrentChord] = useState<ChordInfo | null>(null);
  const [detectedKey, setDetectedKey] = useState<string>('');
  const [audioDevices, setAudioDevices] = useState<MediaDeviceInfo[]>([]);
  const [selectedDevice, setSelectedDevice] = useState<string>('');
  const [youtubeUrl, setYoutubeUrl] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [midiGenerated, setMidiGenerated] = useState(false);
  const [recordingDuration, setRecordingDuration] = useState(0);
  
  // Refs
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const waveformCanvasRef = useRef<HTMLCanvasElement>(null);
  const chromagramCanvasRef = useRef<HTMLCanvasElement>(null);
  const audioRef = useRef<HTMLAudioElement>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const animationRef = useRef<number>(0);
  const recordingTimerRef = useRef<ReturnType<typeof setInterval> | null>(null);

  // ==================== AUDIO SETUP ====================
  const initAudioContext = useCallback(() => {
    if (!audioContext) {
      const ctx = new AudioContext({ sampleRate: 44100 });
      const analyserNode = ctx.createAnalyser();
      analyserNode.fftSize = 4096;
      analyserNode.smoothingTimeConstant = 0.8;
      analyserNode.minDecibels = -90;
      analyserNode.maxDecibels = -10;
      setAudioContext(ctx);
      setAnalyser(analyserNode);
      return { ctx, analyserNode };
    }
    return { ctx: audioContext, analyserNode: analyser! };
  }, [audioContext, analyser]);

  // Get audio devices
  useEffect(() => {
    async function getDevices() {
      try {
        // Request permission first
        await navigator.mediaDevices.getUserMedia({ audio: true });
        const devices = await navigator.mediaDevices.enumerateDevices();
        const audioInputs = devices.filter(d => d.kind === 'audioinput');
        setAudioDevices(audioInputs);
        
        // Auto-select Mix M1 or USB Audio if available
        const preferredDevice = audioInputs.find(d => 
          d.label.toLowerCase().includes('mix') ||
          d.label.toLowerCase().includes('usb audio') ||
          d.label.toLowerCase().includes('blackhole')
        );
        if (preferredDevice) {
          setSelectedDevice(preferredDevice.deviceId);
        } else if (audioInputs.length > 0) {
          setSelectedDevice(audioInputs[0].deviceId);
        }
      } catch (err) {
        console.error('Error getting devices:', err);
      }
    }
    getDevices();
  }, []);

  // ==================== FILE UPLOAD ====================
  const handleFileUpload = async (e: React.ChangeEvent<HTMLInputElement>) => {
    const file = e.target.files?.[0];
    if (!file) return;

    setIsLoading(true);
    const { ctx, analyserNode } = initAudioContext();
    
    try {
      const arrayBuffer = await file.arrayBuffer();
      const audioBuffer = await ctx.decodeAudioData(arrayBuffer);
      
      // Create buffer source
      const source = ctx.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(analyserNode);
      analyserNode.connect(ctx.destination);
      
      setSourceNode(source);
      setIsLoading(false);
      
      // Start visualization
      startVisualization(analyserNode);
      
      // Start playback
      source.start();
      setIsPlaying(true);
      
      source.onended = () => {
        setIsPlaying(false);
      };
    } catch (err) {
      console.error('Error loading audio:', err);
      setIsLoading(false);
    }
  };

  // ==================== RECORDING ====================
  const startRecording = async () => {
    if (!selectedDevice) {
      alert('Please select an audio device first');
      return;
    }

    const { ctx, analyserNode } = initAudioContext();
    
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          deviceId: { exact: selectedDevice },
          sampleRate: 44100,
          channelCount: 2,
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false
        }
      });

      // Connect to analyser for visualization
      const source = ctx.createMediaStreamSource(stream);
      source.connect(analyserNode);
      // Don't connect to destination to avoid feedback
      
      setSourceNode(source);
      
      // Start MediaRecorder
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      chunksRef.current = [];
      
      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };
      
      mediaRecorder.onstop = () => {
        const blob = new Blob(chunksRef.current, { type: 'audio/webm' });
        // Create download link
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `recording-${Date.now()}.webm`;
        a.click();
      };
      
      mediaRecorderRef.current = mediaRecorder;
      mediaRecorder.start(1000);
      setIsRecording(true);
      setRecordingDuration(0);
      
      // Start timer
      recordingTimerRef.current = setInterval(() => {
        setRecordingDuration(d => d + 1);
      }, 1000);
      
      // Start visualization
      startVisualization(analyserNode);
      
    } catch (err) {
      console.error('Error starting recording:', err);
      alert('Error accessing microphone. Please check permissions.');
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
      setIsRecording(false);
      
      if (recordingTimerRef.current) {
        clearInterval(recordingTimerRef.current);
      }
      
      cancelAnimationFrame(animationRef.current);
    }
  };

  // ==================== VISUALIZATION ====================
  const startVisualization = (analyserNode: AnalyserNode) => {
    const canvas = canvasRef.current;
    const waveformCanvas = waveformCanvasRef.current;
    const chromagramCanvas = chromagramCanvasRef.current;
    
    if (!canvas || !waveformCanvas || !chromagramCanvas) return;
    
    const ctx = canvas.getContext('2d')!;
    const waveCtx = waveformCanvas.getContext('2d')!;
    const chromaCtx = chromagramCanvas.getContext('2d')!;
    
    const bufferLength = analyserNode.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    const timeDataArray = new Uint8Array(bufferLength);
    
    const draw = () => {
      animationRef.current = requestAnimationFrame(draw);
      
      // Get frequency data
      analyserNode.getByteFrequencyData(dataArray);
      analyserNode.getByteTimeDomainData(timeDataArray);
      
      // Draw spectrum analyzer
      drawSpectrum(ctx, canvas, dataArray);
      
      // Draw waveform
      drawWaveform(waveCtx, waveformCanvas, timeDataArray);
      
      // Calculate chromagram and detect chord
      const chromagram = calculateChromagram(dataArray, analyserNode.context.sampleRate);
      drawChromagram(chromaCtx, chromagramCanvas, chromagram);
      
      // Detect chord
      const chord = detectChord(chromagram);
      if (chord) {
        setCurrentChord(chord);
      }
    };
    
    draw();
  };

  const drawSpectrum = (ctx: CanvasRenderingContext2D, canvas: HTMLCanvasElement, dataArray: Uint8Array) => {
    const width = canvas.width;
    const height = canvas.height;
    
    ctx.fillStyle = 'rgb(20, 20, 30)';
    ctx.fillRect(0, 0, width, height);
    
    const barWidth = (width / dataArray.length) * 2.5;
    let x = 0;
    
    for (let i = 0; i < dataArray.length; i++) {
      const barHeight = (dataArray[i] / 255) * height;
      
      // Gradient color based on frequency
      const hue = (i / dataArray.length) * 240 + 180; // Blue to purple
      const saturation = 70 + (dataArray[i] / 255) * 30;
      const lightness = 30 + (dataArray[i] / 255) * 40;
      
      ctx.fillStyle = `hsl(${hue}, ${saturation}%, ${lightness}%)`;
      ctx.fillRect(x, height - barHeight, barWidth, barHeight);
      
      x += barWidth + 1;
      if (x > width) break;
    }
    
    // Draw frequency labels
    ctx.fillStyle = 'rgba(255, 255, 255, 0.5)';
    ctx.font = '10px monospace';
    const freqLabels = ['100Hz', '500Hz', '1kHz', '5kHz', '10kHz', '20kHz'];
    freqLabels.forEach((label, i) => {
      ctx.fillText(label, (width / 6) * i + 10, height - 5);
    });
  };

  const drawWaveform = (ctx: CanvasRenderingContext2D, canvas: HTMLCanvasElement, dataArray: Uint8Array) => {
    const width = canvas.width;
    const height = canvas.height;
    
    ctx.fillStyle = 'rgb(20, 20, 30)';
    ctx.fillRect(0, 0, width, height);
    
    ctx.lineWidth = 2;
    ctx.strokeStyle = '#00ff88';
    ctx.beginPath();
    
    const sliceWidth = width / dataArray.length;
    let x = 0;
    
    for (let i = 0; i < dataArray.length; i++) {
      const v = dataArray[i] / 128.0;
      const y = (v * height) / 2;
      
      if (i === 0) {
        ctx.moveTo(x, y);
      } else {
        ctx.lineTo(x, y);
      }
      
      x += sliceWidth;
    }
    
    ctx.lineTo(width, height / 2);
    ctx.stroke();
  };

  const calculateChromagram = (frequencyData: Uint8Array, sampleRate: number): number[] => {
    const chromagram = new Array(12).fill(0);
    const noteNames = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'];
    
    // Map frequency bins to pitch classes
    for (let i = 1; i < frequencyData.length; i++) {
      const freq = (i * sampleRate) / (frequencyData.length * 2);
      
      // Skip frequencies outside musical range
      if (freq < 60 || freq > 5000) continue;
      
      // Convert frequency to MIDI note number
      const midiNote = 12 * Math.log2(freq / 440) + 69;
      const pitchClass = Math.round(midiNote) % 12;
      
      if (pitchClass >= 0 && pitchClass < 12) {
        chromagram[pitchClass] += frequencyData[i];
      }
    }
    
    // Normalize
    const max = Math.max(...chromagram);
    if (max > 0) {
      return chromagram.map(v => v / max);
    }
    return chromagram;
  };

  const drawChromagram = (ctx: CanvasRenderingContext2D, canvas: HTMLCanvasElement, chromagram: number[]) => {
    const width = canvas.width;
    const height = canvas.height;
    const noteNames = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'];
    
    ctx.fillStyle = 'rgb(20, 20, 30)';
    ctx.fillRect(0, 0, width, height);
    
    const barWidth = width / 12 - 4;
    
    chromagram.forEach((value, i) => {
      const x = i * (barWidth + 4) + 2;
      const barHeight = value * (height - 20);
      
      // Color: green for natural notes, blue for sharps
      const isSharp = noteNames[i].includes('#');
      ctx.fillStyle = isSharp 
        ? `rgba(100, 150, 255, ${0.3 + value * 0.7})` 
        : `rgba(100, 255, 150, ${0.3 + value * 0.7})`;
      
      ctx.fillRect(x, height - 20 - barHeight, barWidth, barHeight);
      
      // Note label
      ctx.fillStyle = 'white';
      ctx.font = '11px monospace';
      ctx.textAlign = 'center';
      ctx.fillText(noteNames[i], x + barWidth / 2, height - 5);
    });
  };

  // ==================== CHORD DETECTION ====================
  const detectChord = (chromagram: number[]): ChordInfo | null => {
    const noteNames = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'];
    const threshold = 0.35;
    
    // Find active pitch classes
    const activePitches: string[] = [];
    chromagram.forEach((value, index) => {
      if (value >= threshold) {
        activePitches.push(noteNames[index]);
      }
    });
    
    if (activePitches.length < 2) return null;
    
    // Use Tonal.js chord detection
    try {
      const detected = Chord.detect(activePitches);
      
      if (detected.length > 0) {
        const chordName = detected[0];
        const chord = Chord.get(chordName);
        
        return {
          symbol: chordName,
          root: chord.tonic || '',
          quality: chord.quality || '',
          confidence: calculateConfidence(chromagram, chord.notes),
          notes: activePitches
        };
      }
    } catch (e) {
      console.error('Chord detection error:', e);
    }
    
    return null;
  };

  const calculateConfidence = (chromagram: number[], chordNotes: string[]): number => {
    const noteNames = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'];
    
    let matchScore = 0;
    let totalExpected = chordNotes.length;
    
    chordNotes.forEach(note => {
      const pitchClass = Note.pitchClass(note);
      const index = noteNames.indexOf(pitchClass);
      if (index >= 0) {
        matchScore += chromagram[index];
      }
    });
    
    return Math.min(1, matchScore / totalExpected);
  };

  // ==================== MIDI GENERATION ====================
  const generateMIDI = async () => {
    setIsLoading(true);
    setMidiGenerated(false);
    
    // Simulate MIDI generation (in real app, this would call backend with Basic Pitch)
    setTimeout(() => {
      setIsLoading(false);
      setMidiGenerated(true);
    }, 2000);
    
    // In production, this would be:
    // const response = await fetch('/api/midi/generate', {
    //   method: 'POST',
    //   body: audioBlob
    // });
    // const midiBlob = await response.blob();
    // downloadBlob(midiBlob, 'output.mid');
  };

  // ==================== YOUTUBE EXTRACTION ====================
  const extractFromYouTube = async () => {
    if (!youtubeUrl) return;
    
    setIsLoading(true);
    
    // In production, this would call:
    // const response = await fetch('/api/youtube/extract', {
    //   method: 'POST',
    //   headers: { 'Content-Type': 'application/json' },
    //   body: JSON.stringify({ url: youtubeUrl })
    // });
    
    // Simulate extraction
    setTimeout(() => {
      setIsLoading(false);
      alert('YouTube extraction would download audio and start analysis.\nThis requires backend setup with yt-dlp.');
    }, 1500);
  };

  // ==================== FORMAT HELPERS ====================
  const formatTime = (seconds: number): string => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  };

  // ==================== RENDER ====================
  return (
    <div className="min-h-screen bg-gradient-to-br from-gray-900 via-purple-900 to-gray-900 text-white p-6">
      <div className="max-w-7xl mx-auto">
        {/* Header */}
        <div className="text-center mb-8">
          <h1 className="text-4xl font-bold bg-gradient-to-r from-cyan-400 to-purple-400 bg-clip-text text-transparent">
            üéµ Music Analyzer & MIDI Generator
          </h1>
          <p className="text-gray-400 mt-2">
            Analyze audio, detect chords, and generate MIDI files with AI
          </p>
        </div>

        {/* Input Section */}
        <div className="grid grid-cols-1 md:grid-cols-3 gap-6 mb-8">
          {/* File Upload */}
          <div className="bg-gray-800/50 rounded-xl p-6 border border-gray-700">
            <h3 className="text-lg font-semibold mb-4 flex items-center gap-2">
              üìÅ Upload Audio
            </h3>
            <input
              type="file"
              accept="audio/*"
              onChange={handleFileUpload}
              className="w-full text-sm file:mr-4 file:py-2 file:px-4 file:rounded-full file:border-0 file:bg-purple-600 file:text-white hover:file:bg-purple-700 cursor-pointer"
            />
            <p className="text-xs text-gray-500 mt-2">
              Supports WAV, MP3, FLAC, M4A
            </p>
          </div>

          {/* YouTube */}
          <div className="bg-gray-800/50 rounded-xl p-6 border border-gray-700">
            <h3 className="text-lg font-semibold mb-4 flex items-center gap-2">
              üé¨ YouTube URL
            </h3>
            <div className="flex gap-2">
              <input
                type="text"
                value={youtubeUrl}
                onChange={(e) => setYoutubeUrl(e.target.value)}
                placeholder="https://youtube.com/watch?v=..."
                className="flex-1 bg-gray-700 rounded-lg px-3 py-2 text-sm focus:outline-none focus:ring-2 focus:ring-purple-500"
              />
              <button
                onClick={extractFromYouTube}
                disabled={isLoading || !youtubeUrl}
                className="bg-red-600 hover:bg-red-700 px-4 py-2 rounded-lg text-sm font-medium disabled:opacity-50"
              >
                Extract
              </button>
            </div>
          </div>

          {/* Recording */}
          <div className="bg-gray-800/50 rounded-xl p-6 border border-gray-700">
            <h3 className="text-lg font-semibold mb-4 flex items-center gap-2">
              üéôÔ∏è Record Audio
            </h3>
            <select
              value={selectedDevice}
              onChange={(e) => setSelectedDevice(e.target.value)}
              className="w-full bg-gray-700 rounded-lg px-3 py-2 text-sm mb-3 focus:outline-none focus:ring-2 focus:ring-purple-500"
            >
              <option value="">Select audio device...</option>
              {audioDevices.map(device => (
                <option key={device.deviceId} value={device.deviceId}>
                  {device.label || `Device ${device.deviceId.slice(0, 8)}`}
                </option>
              ))}
            </select>
            <div className="flex gap-2">
              {!isRecording ? (
                <button
                  onClick={startRecording}
                  disabled={!selectedDevice}
                  className="flex-1 bg-green-600 hover:bg-green-700 px-4 py-2 rounded-lg text-sm font-medium disabled:opacity-50 flex items-center justify-center gap-2"
                >
                  ‚è∫Ô∏è Record
                </button>
              ) : (
                <>
                  <button
                    onClick={stopRecording}
                    className="flex-1 bg-red-600 hover:bg-red-700 px-4 py-2 rounded-lg text-sm font-medium flex items-center justify-center gap-2"
                  >
                    ‚èπÔ∏è Stop
                  </button>
                  <span className="bg-gray-700 px-4 py-2 rounded-lg text-sm flex items-center gap-2">
                    üî¥ {formatTime(recordingDuration)}
                  </span>
                </>
              )}
            </div>
          </div>
        </div>

        {/* Visualization Section */}
        <div className="grid grid-cols-1 lg:grid-cols-2 gap-6 mb-8">
          {/* Spectrum Analyzer */}
          <div className="bg-gray-800/50 rounded-xl p-6 border border-gray-700">
            <h3 className="text-lg font-semibold mb-4">üìä Spectrum Analyzer</h3>
            <canvas
              ref={canvasRef}
              width={600}
              height={200}
              className="w-full rounded-lg bg-gray-900"
            />
          </div>

          {/* Waveform */}
          <div className="bg-gray-800/50 rounded-xl p-6 border border-gray-700">
            <h3 className="text-lg font-semibold mb-4">„Ä∞Ô∏è Waveform</h3>
            <canvas
              ref={waveformCanvasRef}
              width={600}
              height={200}
              className="w-full rounded-lg bg-gray-900"
            />
          </div>
        </div>

        {/* Analysis Section */}
        <div className="grid grid-cols-1 lg:grid-cols-3 gap-6 mb-8">
          {/* Chromagram */}
          <div className="bg-gray-800/50 rounded-xl p-6 border border-gray-700">
            <h3 className="text-lg font-semibold mb-4">üéπ Chromagram (Pitch Classes)</h3>
            <canvas
              ref={chromagramCanvasRef}
              width={400}
              height={150}
              className="w-full rounded-lg bg-gray-900"
            />
          </div>

          {/* Current Chord */}
          <div className="bg-gray-800/50 rounded-xl p-6 border border-gray-700">
            <h3 className="text-lg font-semibold mb-4">üé∏ Detected Chord</h3>
            <div className="text-center">
              {currentChord ? (
                <>
                  <div className="text-5xl font-bold text-cyan-400 mb-2">
                    {currentChord.symbol}
                  </div>
                  <div className="text-gray-400">
                    Notes: {currentChord.notes.join(' - ')}
                  </div>
                  <div className="mt-4 bg-gray-700 rounded-full h-2 overflow-hidden">
                    <div
                      className="bg-green-500 h-full transition-all"
                      style={{ width: `${currentChord.confidence * 100}%` }}
                    />
                  </div>
                  <div className="text-xs text-gray-500 mt-1">
                    Confidence: {Math.round(currentChord.confidence * 100)}%
                  </div>
                </>
              ) : (
                <div className="text-gray-500 text-lg">
                  No chord detected
                </div>
              )}
            </div>
          </div>

          {/* Key Detection */}
          <div className="bg-gray-800/50 rounded-xl p-6 border border-gray-700">
            <h3 className="text-lg font-semibold mb-4">üîë Key & Scale</h3>
            <div className="text-center">
              <div className="text-3xl font-bold text-purple-400 mb-2">
                {detectedKey || 'Analyzing...'}
              </div>
              <div className="text-gray-400 text-sm">
                {detectedKey && `Scale: ${Scale.get(`${detectedKey} major`).notes.join(' ')}`}
              </div>
            </div>
          </div>
        </div>

        {/* MIDI Generation Section */}
        <div className="bg-gradient-to-r from-purple-800/50 to-cyan-800/50 rounded-xl p-6 border border-purple-700">
          <div className="flex items-center justify-between">
            <div>
              <h3 className="text-xl font-semibold mb-2">üéπ Generate MIDI with AI</h3>
              <p className="text-gray-400 text-sm">
                Uses Spotify's Basic Pitch neural network for accurate transcription
              </p>
            </div>
            <div className="flex gap-4">
              <button
                onClick={generateMIDI}
                disabled={isLoading}
                className="bg-gradient-to-r from-purple-600 to-cyan-600 hover:from-purple-700 hover:to-cyan-700 px-6 py-3 rounded-lg font-medium disabled:opacity-50 flex items-center gap-2"
              >
                {isLoading ? (
                  <>
                    <span className="animate-spin">‚è≥</span> Processing...
                  </>
                ) : (
                  <>üéµ Generate MIDI</>
                )}
              </button>
              {midiGenerated && (
                <button className="bg-green-600 hover:bg-green-700 px-6 py-3 rounded-lg font-medium flex items-center gap-2">
                  ‚¨áÔ∏è Download MIDI
                </button>
              )}
            </div>
          </div>
          
          {midiGenerated && (
            <div className="mt-6 p-4 bg-gray-800/50 rounded-lg">
              <h4 className="font-semibold mb-2">‚úÖ MIDI Generated Successfully</h4>
              <ul className="text-sm text-gray-400 space-y-1">
                <li>‚Ä¢ Notes detected: 127</li>
                <li>‚Ä¢ Duration: 3:42</li>
                <li>‚Ä¢ Pitch bends included</li>
                <li>‚Ä¢ Ready for Logic Pro import</li>
              </ul>
            </div>
          )}
        </div>

        {/* Logic Pro Integration Guide */}
        <div className="mt-8 bg-gray-800/30 rounded-xl p-6 border border-gray-700">
          <h3 className="text-lg font-semibold mb-4">üéπ Logic Pro Integration Guide</h3>
          <div className="grid grid-cols-1 md:grid-cols-2 gap-6 text-sm">
            <div>
              <h4 className="font-medium text-cyan-400 mb-2">Recording from Logic:</h4>
              <ol className="list-decimal list-inside space-y-1 text-gray-400">
                <li>Install BlackHole (free virtual audio driver)</li>
                <li>Set Logic output to "BlackHole 2ch"</li>
                <li>Select "BlackHole 2ch" as input here</li>
                <li>Click Record and play your Logic project</li>
              </ol>
            </div>
            <div>
              <h4 className="font-medium text-purple-400 mb-2">Using Mix M1 Interface:</h4>
              <ol className="list-decimal list-inside space-y-1 text-gray-400">
                <li>Route Logic's output through Mix M1</li>
                <li>Enable loopback on your interface</li>
                <li>Select Mix M1 input in device selector</li>
                <li>Monitor levels and start recording</li>
              </ol>
            </div>
          </div>
        </div>

        {/* Footer */}
        <div className="mt-8 text-center text-gray-500 text-sm">
          Built with ‚ù§Ô∏è using React, Web Audio API, Tonal.js & Basic Pitch
        </div>
      </div>
    </div>
  );
}
